\chapter{General Discussion} 
\label{discussion} 
\lhead{Chapter 5. \emph{General Discussion}} 


The questions that we aimed to investigate in this thesis were i) Is the spatial information of the target of reach action represented in an egocentric frame of reference, that is, with respect to region of the body in proximity to it? ii)If yes, and considering that visuo-proprioceptive integration mechanism underlies construction of body schema, 


what are the mechanisms underlying the spatial encoding of the target with respect to the proximal body-part?

We hypothesized that the mechanism of multi-sensory integration underlying the construction of body-schema, may also underlie the spatial encoding of reach action target situated in its proximity. To investigate this hypothesis, we experimentally manipulated the presence of visual and proprioceptive information of a body-part proximal to the reach action target, and observed the accuracy of the target location estimated by the subjects in reaching action. 

One result from the first experiment show that when multi-sensory visual and proprioceptive information of the proximal body part is present, there is greater accuracy in target location estimation, as compared to the situation in which visual and proprioceptive information of the proximal body part is absent, that is, there is absence of the proximal body-part itself. This result thus supports the hypothesis that spatial information of the target is indeed represented in an egocentric reference frame, with respect to the body part proximal to it. Notably, this body-part need not the effector used in the undertaking of the reach action, but should be merely situated in the proximity of the reach target. %%%%%%Somewhere i read that, proprioceptive information is used for something and visual for something, i think it was Van Beers. and i think it was about effector %%%%%%%%%

Moreover, second result from Experiment 1 shows that target location estimation is not improved significantly when only uni-sensory (visual or proprioceptive) information of the proximal body part is present. This result provides support to the hypothesis that integration of multi-sensory signals regarding the proximal body-part is the mechanism underlying the spatial encoding of reach action target, and that integration of multi-sensory information is necessary for encoding of the target's spatial information with respect to the proximal body-part.

If multi-sensory integration of the proximal body-region is indeed the mechanism underlying the representation of the target, how does inducing spatial discrepancy between the visual and the proprioceptive inputs affect the representation of the target? The results show that the estimation of target location is increasingly "pulled" towards the proximal body-part, as the distance between the visual hand and the target decreases, irrespective of the magnitude of spatial discrepancy between the visual and the proprioceptive input. As elaborated upon in Chapter \ref{exp2}, these results suggest that the spatial encoding of the target is with respect to the spatial information specified by the visual input, as opposed to the multi-modal estimate of the proximal body-part position. 

While the results of the two experiments seemed contradictory, that is first experiment seems to provide support for multi-sensory integration for target spatial encoding, while the second provides support for visual uni-sensory input, the difference is that multi-sensory integration does seem to be necessary, as shown by experiment 1. In the second experiment, while the spatial encoding seems to be occurring with respect to the visual input, multi-sensory integration processes is occurring in all situations, despite the spatial discrepancies in the multi-modal inputs. Thus, taking the results of the two experiments together, consider the following explanation:
1. Target is encoded with respect to visual input of surrounding objects, in an alocentric frame of reference
2. However, multi-sensory integration processes lead to greater accuracy in encoding.

Another result is that there is an overall under-estimation in the reaches. 

We cannot say anything more. Further experiments are required to probe further into at what stage of integration does the spatial encoding occur. Our results point to the hypothesis that the encoding happens with uni-sensory inputs, before the multi-sensory integration process occurs.

Overall, our results suggest that in the target encoding mechanism, the target is encoded with visual information which is one of the inputs of the multi-sensory integration process which underlies the construction of body schema. 








Third result : effect of position. We were confused by this result, that its not interacting....
Interaction is not surprising, because as the distance between AH and T increases, the distance between VH and T decreases. and 


Visual landmark studies suggest allocentric
Studies have demonstrated more accurate reaching in presence of visual landmarks surrounding the object. 
Byrne, P. A., and Crawford, J. D. (2010). Cue reliability and a landmark stability heuristic determine relative weighting between egocentric and allocentric visual information in memory-guided reach. J. Neurophysiol. 103, 3054–3069. doi: 10.1152/jn.01008.2009


Evidence suggests that reaching is performed in eye-centered co-ordinates.

One possible interpretation of this landmark influence is that egocentric and allocentric spatial representations are combined (Schütz et al., 2013). Schütz, I., Henriques, D. Y. P., and Fiehler, K. (2013). Gaze-centered spatial updating in delayed reaching even in the pr


body is an object of perception for visual modality. Therefore, it is considered a part of the environment before it is integrated with proprioceptive information. 


So, there is evidence that reach targets are represented with respect to landmarks. 
However, it is combined with eye-centered reference frames (Shultz, 2013). 

1. Evidence for alocentric representation wrt visual landmarks. (review filimon, 2015)
2. Evidence for integration of alocentric and ego centric frames, though eye centered. (Schutz, 2013)
3. Proposal that visual and proprioceptive information is used for different purposes, though its for acting hand. (Vanbeers 1996), Jeonnard, but reviewed by Vanden
4. One hypothesis: visual and proprioception used for different stages in action control. Explanation could be that visual is used for encoding spatial information, while proprioception is used for motor accuracy.
5. 




\begin{itemize}
    \item Results from experiment 1 support the idea that target of reach action is represented with respect to the body, and that outcome of reach action is affected by this body-part centered spatial encoding.
    \item Furthermore, when both proprioceptive and visual inputs of proximal body-part are present, target location estimation is more accurate.
    \item However, When both are present, and congruent, there is still some inaccuracy, as suggested by both Experiment 1 (PV condition) and Experiment 2 (discrepancy = 0 condition). When there is uncertainty, which we induced by making the action hand invisible, the estimate of the target is pulled towards the action hand (refer to the main effect of position from experiment 2).
    \item The error in estimation can be further minimized by creating discrepancy in the visuo-proprioceptive inputs by bringing estimate of the visual hand closer to the target. Why? What is the mechanistic explanation of this?
    \item However, the visuo-proprioceptive discrepancy does increase the reach error in certain cases, as evidenced by reach error values in negative discrepancy conditions. The reach error magnitude is high and towards the action hand. So, it's not just that the visual hand is simply pulling the estimate of the target towards it. 

\end{itemize}


The brain uses information from multiple sensory modalities to construct a flexible representation of the body, including its current structure and position (Dijkerman and de Haan, 2007; Makin et al., 2008; Tsakiris, 2010; Blanke, 2012; Ehrsson, 2012). For such a body representation, vision and proprioception continually provide estimates of (changing) limb position, which is crucial to guide actions (Wolpert et al., 1998; Graziano and Botvinick, 2002; Holmes and Spence, 2004). Behavioral experiments using prisms or mirrors have shown that visuo-proprioceptive limb position information is integrated based on the relative reliability of the unisensory modalities, with vision usually “dominating” proprioception due to its higher spatial acuity (van Beers et al., 1999; Holmes and Spence, 2005). This is also suggested by the rubber hand illusion (RHI) (Botvinick and Cohen, 1998), in which visual position information from a fake hand “overrides” proprioception after visuo-tactile costimulation of the fake hand and the real hand. Notably, the RHI only works if the fake hand is in a position similar to the real hand (Pavani et al., 2000; Ehrsson et al., 2004; Lloyd, 2007; Makin et al., 2008), which implies that the brain only integrates visuo-proprioceptive information under sufficient cross-modal congruence.

https://www.jneurosci.org/content/36/9/2582

