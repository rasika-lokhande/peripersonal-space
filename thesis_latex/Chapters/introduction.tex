\chapter{Introduction} 
\label{introduction} 
\lhead{Chapter 1. \emph{Introduction}} 


You extend your arm to grab a cup of coffee, grasp the book you're holding, or swat a mosquito buzzing around your ears. One of the most common ways we interact with our surroundings is by reaching out with our hands to touch the objects in our surroundings. Reaching is one of the fundamental human actions involved in human-environment interaction, underpinning diverse abilities such as feeding, object manipulation, and self-defense. A characteristic feature of reaching action is that it is goal-directed towards a specific external target. Once the target is specified during the action selection stage, it must be translated into a movement plan so that the effector (the hand) can make contact with the target(eg. the coffee mug). To accomplish this, \textit{the spatial location} of the coffee mug must be encoded in a format suitable for motor control. Information from various sensory modalities about the spatial location of the target have to be integrated with the spatial information of the effector to perform an accurate reach. How is the spatial information of the reach target encoded? In particular, what is the reference frame in which the reach target is spatially encoded? 

%You reach your arm out to grab a cup of coffee, to grasp the book in your hand, or to swat off the mosquito buzzing around your ears.  One of the most common ways we interact with the environment is through reaching out our hands in order to make contact with the objects in our environment. Reaching action is one of the fundamental human actions involved in human-environment interaction, underlying various capabilities like feeding, object manipulation, or self-defense. A characteristic of reaching action is that it is goal directed towards a specific target in the external environment. Once the target is specified in the action selection stage, this goal must be transformed into a movement plan, so that the effector (the hand) can make contact with the target (the coffee cup). For this purpose, the spatial location of the cup of coffee must be encoded, in a format appropriate for motor control. Information from various sensory modalities about the spatial location of the target have to be integrated with the spatial information of the effector to perform an accurate reach. How is the spatial information of the reach target encoded? In particular, what is the reference frame in which the reach target is spatially encoded? 

%The region of space immediately surrounding the body is termed as peri-personal space. A distinction is made between peri-personal space and extra-personal space, because of certain behavioural and neuro-physiological effects observed when objects are present in peri-personal space. For instance, 

The targets of reaching action are situated in the region of in close proximity to the body, which is termed in the literature as \emph{peri-personal space}. The characteristic feature of representation of spatial information of objects in peri-personal space is that it is encoded with respect to the area of body which is in proximity to the object \cite{serino2019peripersonal}.
 This notion that the representation of spatial information of the object is "anchored" to the proximal body-part is supported by both neuro-physiological and behavioural evidence (See Chapter \ref{Literature Review}). For instance, visuo-tactile multi-sensory interaction effects such as the \emph{cross-modal congruency effect} has been observed in certain experiments, wherein subjects localize tactile stimuli faster when a task-irrelevant visual cue is presented congruent to that body part, as compared to the condition where the task irrelevant cue is presented incongruent to the tactile stimulated body part \cite{spence2004multisensory}. The magnitude of cross-modal congruency effect (and other similar multi-sensory interaction effects) is also indicative of the distance between the stimuli and the body \cite{spence2004spatial, iachini2014motor}, that is, these multi-sensory responses increase as the distance between the visual stimuli and the body decreases. 
 
 %These results are indicative of two notions - i) Visual objects in peri-personal space are spatially with respect to the proximal body-part, and ii) Multi-sensory 

 These multi-sensory interaction effects are used in literature as a proxy for understanding how peri-personal space in implicated in various cognitive processes. For instance, a functional link has been demonstrated between action and multi-sensory behavioural responses evoked by objects in peri-personal space \cite{ladavas2008action, patane2019action, brozzoli2010action}, suggesting that the peri-personal space representation serves as a perceptual-motor interface for guiding goal directed actions. Furthermore, the multi-sensory interaction effects are also modulated by action possibilities \cite{lohmann2019hands, iriki1996coding, senna2019aim}. It has thus been theorized that a generative mechanism underlies representation of peri-personal space, which predicts the likelihood of future sensory outcomes \cite{lohmann2019hands, noel2018peri}. In this light, the representation of spatial information of objects in peri-personal space could be interpreted as a representation of space in terms of a constantly updated set of probabilities that carry information about the likelihood that stimuli in the space around the body will make contact with a specific area of the body. If this is indeed the case, modulating parameters that affect the representation of peri-personal space should affect the outcome of actions involving contact, that is, reaching actions. However, one of the gaps in the literature is that the link between goal directed action and peri-personal space- where the reach action targets are present, has only been investigated through the multi-sensory interaction effects paradigm, and not through the outcome of reach action- that is, how accurate were the subjects in estimating the location of the target, when factors affecting peri-personal space representations are modulated.

 If the reach targets are spatially encoded with respect to body-part proximal to them, it is plausible that the representation of body parameters is also involved in the reach targets' spatial encoding. One of the views in the literature is that the cognitive system represents body parameters, particularly those necessary for motor control- including the spatial information of the effector, in an integrated representation termed as \emph{body schema} \cite{ataria2021body}. Notably, the construction of body schema is commonly conceptualized as an inference problem, where most probable estimates of the body parameters (here, the spatial position of the body-part) are dynamically computed from the inflow of sensory inputs from various sensory modalities, like vision and proprioception \cite{van1999integration}. In other words, a visuo-proprioceptive integrative mechanism underlies the specification of spatial information in body schema. Could the same visuo-proprioceptive integration mechanism which specifies the spatial information of the body-part, also underlie the spatial encoding of the reach target which lies in proximity to this body-part?

 To investigate this question, two experiments were conducted in which a reaching action task paradigm was implemented in an immersive virtual reality environment. With their action hands rendered invisible, the subjects were asked to make contact with a visual target, while visual and proprioceptive information of the action-irrelevant body part proximal to the target was manipulated, and accuracy of target location estimated by the participant was measured. In the first experiment, we investigated whether the reach target location estimation is indeed affected by the presence or absence of a proximal body-part, and if so, how does the uni-sensory and multi-sensory information of the proximal body-part affect the reach target location estimation. In the second experiment, we attempted to probe further into how visuo-proprioceptive integration mechanism underlies the spatial encoding of reach target, by manipulating the spatial discrepancies between the visual and proprioceptive information of the proximal body-part. The results of the two experiments support the hypothesis that reach target is encoded with respect to the proximal body-part, and that its multi-sensory integration mechanism underlies the reach action target encoding. However, the results suggest that the spatial encoding of reach target occurs relative to the location specified by the visual information of the proximal body-part, as opposed to the location of the body-part inferred after the multi-sensory integration process.


 

 %The objective of this thesis is to understand how the integration of visuo-proprioceptive information regarding the body in proximity to the reach action target underlies the encoding of its spatial information. To investigate this question, two experiments were conducted. In both experiments, a reaching action task was implemented in an immersive virtual environment, in which the subjects were asked to make contact with a target, with their action hand rendered invisible. The visual and proprioceptive information of a body-part which is in proximity to the target, but not directly relevant to the task of reaching action, was manipulated, while the end point of the reaching action was measured as the subject's estimated target location. We expected that the discrepancies between the spatial estimates provided by visual and proprioceptive input regarding the position of the proximal body-part should affect the spatial representation of the target, which should be observable in the estimation of target location in the reaching action. The aim of the first experiment was to seek empirical evidence supporting the notion that spatial encoding of target is affected by sensory information regarding the body-part which is in proximity to it, by investigating the differences in the target location estimation accuracy between visual and proprioceptive uni-sensory and multi-sensory inputs of the proximal body-part.  In the second experiment, we probed further into the integration mechanism by inducing systematic spatial discrepancies between the visual and proprioceptive inputs to understand its effect on the target location estimation. Based on the results of the two experiments, we find evidence suggesting that the sensory inputs about the proximal body-part does indeed underlie the spatial encoding of the reach target. Furthermore, the spatial representation seems to be predominantly anchored to the visual information of the proximal body part. However, the integration of the visual input with proprioception may be necessary for the the anchoring to occur more accurately.


%One of the views in the literature is that the cognitive system represents body parameters essential for motor control, including the spatial information of the effector, in an integrated representation termed as \emph{body schema} \cite{ataria2021body}. The construction of body schema is commonly conceptualized as an inference problem, where most probable estimates of the body parameters are dynamically computed from the inflow of sensory inputs from various sensory modalities, like vision and proprioception \cite{van1999integration}, on the basis of the reliability of the sensory input (noise), and the prior beliefs encoded in the system regarding the relevance of the sensory inputs. Thus, a visuo-proprioceptive integrative mechanism underlies the specification of spatial information in body schema, which is utilized for planning and guiding the reaching action.


%Thus, body-part anchoring and body-object proximity dependence are two important properties associated with how the spatial information of objects in peri-personal space is represented. Moreover, contact-location prediction in goal-directed action is one of the functional explanation to the nature of peri-personal space representation. Thus, the two properties of spatial encoding of objects - the body-part anchoring and the body-object proximity dependence, should also be involved in the outcome of actions involving contact. The body-part anchoring also suggests that not only does the representation of bodily parameters specified in body schema provide information about the state of the effector, but that body schema may also be involved in spatial encoding of action target and specifying the target's spatial location with respect to the body. Since visuo-proprioceptive integrative mechanisms underlie construction of body schema, disturbances in the multi-sensory integration process should affect the spatial encoding of the target in its proximity. Therefore, visuo-proprioceptive discrepancies in the body-part proximal to the target should affect the estimation of target location in reaching action.

%To investigate this conjecture, a reaching action task was implemented in an immersive virtual environment, in which visual and proprioceptive information of a body-part which is in proximity to the target, but not directly relevant to the task of reaching action, was manipulated. We expected that the visuo-proprioceptive discrepancies of the body should affect the spatial representation of the target, which should be observable in the estimation of target location in the reaching action.

%The objective of this thesis is to understand how the integration of visuo-proprioceptive information regarding the body in proximity to the reach action target underlies the encoding of its spatial information. To investigate this question, two experiments were conducted. In both experiments, a reaching action task was implemented in an immersive virtual environment, in which the subjects were asked to make contact with a target, with their action hand rendered invisible. The visual and proprioceptive information of a body-part which is in proximity to the target, but not directly relevant to the task of reaching action, was manipulated, while the end point of the reaching action was measured as the subject's estimated target location. We expected that the discrepancies between the spatial estimates provided by visual and proprioceptive input regarding the position of the proximal body-part should affect the spatial representation of the target, which should be observable in the estimation of target location in the reaching action. The aim of the first experiment was to seek empirical evidence supporting the notion that spatial encoding of target is affected by sensory information regarding the body-part which is in proximity to it, by investigating the differences in the target location estimation accuracy between visual and proprioceptive uni-sensory and multi-sensory inputs of the proximal body-part.  In the second experiment, we probed further into the integration mechanism by inducing systematic spatial discrepancies between the visual and proprioceptive inputs to understand its effect on the target location estimation. Based on the results of the two experiments, we find evidence suggesting that the sensory inputs about the proximal body-part does indeed underlie the spatial encoding of the reach target. Furthermore, the spatial representation seems to be predominantly anchored to the visual information of the proximal body part. However, the integration of the visual input with proprioception may be necessary for the the anchoring to occur more accurately.

